{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Symbol                 Date       Open      Close       High  \\\n",
      "0          RPD  2018-01-02 00:00:00  18.660000  19.010000  19.090000   \n",
      "1          RPD  2018-01-03 00:00:00  19.040001  19.350000  19.650000   \n",
      "2          RPD  2018-01-04 00:00:00  19.389999  19.980000  20.000000   \n",
      "3          RPD  2018-01-05 00:00:00  20.000000  20.010000  20.100000   \n",
      "4          RPD  2018-01-08 00:00:00  20.020000  20.350000  20.500000   \n",
      "...        ...                  ...        ...        ...        ...   \n",
      "5177034    RNG  2023-03-14 00:00:00  32.180000  31.590000  32.680000   \n",
      "5177035    OGI  2023-03-15 00:00:00   0.650000   0.640000   0.651000   \n",
      "5177036    RNG  2023-03-15 00:00:00  31.320000  32.380001  32.549999   \n",
      "5177037    OGI  2023-03-16 00:00:00   0.640000   0.649000   0.667000   \n",
      "5177038    RNG  2023-03-16 00:00:00  32.419998  31.969999  32.580002   \n",
      "\n",
      "               Low     Volume   AdjClose  \n",
      "0        18.500000   124200.0  19.010000  \n",
      "1        19.040001   204100.0  19.350000  \n",
      "2        19.389999   177900.0  19.980000  \n",
      "3        19.719999   204400.0  20.010000  \n",
      "4        19.950001   237100.0  20.350000  \n",
      "...            ...        ...        ...  \n",
      "5177034  31.230000  2080300.0  31.590000  \n",
      "5177035   0.629000   822400.0   0.640000  \n",
      "5177036  30.990000  1694200.0  32.380001  \n",
      "5177037   0.629000  1386100.0   0.649000  \n",
      "5177038  31.180000  1495200.0  31.969999  \n",
      "\n",
      "[5177039 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('history.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Symbol       Date       Open      Close       High        Low  \\\n",
      "0          RPD 2018-01-02  18.660000  19.010000  19.090000  18.500000   \n",
      "1          RPD 2018-01-03  19.040001  19.350000  19.650000  19.040001   \n",
      "2          RPD 2018-01-04  19.389999  19.980000  20.000000  19.389999   \n",
      "3          RPD 2018-01-05  20.000000  20.010000  20.100000  19.719999   \n",
      "4          RPD 2018-01-08  20.020000  20.350000  20.500000  19.950001   \n",
      "...        ...        ...        ...        ...        ...        ...   \n",
      "5177034    RNG 2023-03-14  32.180000  31.590000  32.680000  31.230000   \n",
      "5177035    OGI 2023-03-15   0.650000   0.640000   0.651000   0.629000   \n",
      "5177036    RNG 2023-03-15  31.320000  32.380001  32.549999  30.990000   \n",
      "5177037    OGI 2023-03-16   0.640000   0.649000   0.667000   0.629000   \n",
      "5177038    RNG 2023-03-16  32.419998  31.969999  32.580002  31.180000   \n",
      "\n",
      "            Volume   AdjClose  Year  Month  DayOfWeek  WeekOfYear  \n",
      "0         124200.0  19.010000  2018      1          1           1  \n",
      "1         204100.0  19.350000  2018      1          2           1  \n",
      "2         177900.0  19.980000  2018      1          3           1  \n",
      "3         204400.0  20.010000  2018      1          4           1  \n",
      "4         237100.0  20.350000  2018      1          0           2  \n",
      "...            ...        ...   ...    ...        ...         ...  \n",
      "5177034  2080300.0  31.590000  2023      3          1          11  \n",
      "5177035   822400.0   0.640000  2023      3          2          11  \n",
      "5177036  1694200.0  32.380001  2023      3          2          11  \n",
      "5177037  1386100.0   0.649000  2023      3          3          11  \n",
      "5177038  1495200.0  31.969999  2023      3          3          11  \n",
      "\n",
      "[5177039 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Add in special date columns\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df.apply(lambda row: row['Date'].year, axis=1)\n",
    "df['Month'] = df.apply(lambda row: row['Date'].month, axis=1)\n",
    "df['DayOfWeek'] = df.apply(lambda row: row['Date'].weekday(), axis=1)\n",
    "df['WeekOfYear'] = df.apply(lambda row: row['Date'].isocalendar()[1], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Symbol', 'Date', 'Open', 'Close', 'High', 'Low', 'Volume', 'AdjClose',\n",
      "       'Year', 'Month', 'DayOfWeek', 'WeekOfYear', 'sin_DayOfWeek',\n",
      "       'cos_DayOfWeek', 'sin_Month', 'cos_Month', 'sin_WeekOfYear',\n",
      "       'cos_WeekOfYear'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_cyclical_features(df, col_name, period, start_num=0):\n",
    "    kwargs = {\n",
    "        f'sin_{col_name}' : lambda x: np.sin(2*np.pi*(df[col_name]-start_num)/period),\n",
    "        f'cos_{col_name}' : lambda x: np.cos(2*np.pi*(df[col_name]-start_num)/period)    \n",
    "             }\n",
    "    return df.assign(**kwargs)\n",
    "\n",
    "df = generate_cyclical_features(df, 'DayOfWeek', 7, 0)\n",
    "df = generate_cyclical_features(df, 'Month', 12, 1)\n",
    "df = generate_cyclical_features(df, 'WeekOfYear', 52, 0)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Symbol', 'Date', 'Open', 'Close', 'High', 'Low', 'Volume', 'AdjClose',\n",
      "       'Year', 'Month', 'DayOfWeek', 'WeekOfYear', 'sin_DayOfWeek',\n",
      "       'cos_DayOfWeek', 'sin_Month', 'cos_Month', 'sin_WeekOfYear',\n",
      "       'cos_WeekOfYear', 'is_holiday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import holidays\n",
    "us_holidays = holidays.US()\n",
    "\n",
    "def is_holiday(date):\n",
    "    date = date.replace(hour = 0)\n",
    "    return 1 if (date in us_holidays) else 0\n",
    "\n",
    "def add_holiday_col(df, holidays):\n",
    "    return df.assign(is_holiday = df['Date'].apply(is_holiday))\n",
    "\n",
    "df = add_holiday_col(df, us_holidays)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert symbol to number\n",
    "symbol_map = {}\n",
    "symbol_map_rev = {}\n",
    "\n",
    "symbols = df['Symbol'].unique()\n",
    "for i, symbol in enumerate(symbols):\n",
    "    symbol_map[symbol] = i\n",
    "    symbol_map_rev[i] = symbol\n",
    "\n",
    "df['Symbol_Num'] = df.apply(lambda row: symbol_map[row['Symbol']], axis=1)\n",
    "df = df.drop('Symbol', axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Split into Training and Test sets\n",
    "\n",
    "Do this by breaking data up into date ranges, as we are mirroing a live setup where we have the full \n",
    "past data, and need to predict the future. We are not trying to do things like, given random selctions\n",
    "of dates in the past predict a future date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'Close', 'High', 'Low', 'Volume', 'AdjClose', 'Year',\n",
       "       'sin_DayOfWeek', 'cos_DayOfWeek', 'sin_Month', 'cos_Month',\n",
       "       'sin_WeekOfYear', 'cos_WeekOfYear', 'is_holiday', 'Symbol_Num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = df[(df['Year']==2018) & (df['Month'] < 12)]\n",
    "train_y = df[(df['Year']==2018) & (df['Month'] == 12)]\n",
    "train_y = train_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "val_x = df[(df['Year']==2019) & (df['Month'] < 6)]\n",
    "val_y = df[(df['Year']==2019) & (df['Month'] == 6)]\n",
    "val_y = val_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "test_x = df[(df['Year']==2019) & (df['Month'] < 12) & (df['Month'] > 6)]\n",
    "test_y = df[(df['Year']==2019) & (df['Month'] == 12)]\n",
    "test_y = test_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "cols_to_drop = ['Month', 'Date', 'DayOfWeek', 'WeekOfYear']\n",
    "train_x = train_x.drop(cols_to_drop, axis=1)\n",
    "test_x = test_x.drop(cols_to_drop, axis=1)\n",
    "val_x = val_x.drop(cols_to_drop, axis=1)\n",
    "\n",
    "train_x.columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_arr = scaler.fit_transform(train_x)\n",
    "X_val_arr = scaler.transform(val_x)\n",
    "X_test_arr = scaler.transform(test_x)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(train_y)\n",
    "y_val_arr = scaler.transform(val_y)\n",
    "y_test_arr = scaler.transform(test_y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m test_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(X_test_arr)\n\u001b[1;32m     11\u001b[0m test_targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(y_test_arr)\n\u001b[0;32m---> 13\u001b[0m train \u001b[39m=\u001b[39m TensorDataset(train_features, train_targets)\n\u001b[1;32m     14\u001b[0m val \u001b[39m=\u001b[39m TensorDataset(val_features, val_targets)\n\u001b[1;32m     15\u001b[0m test \u001b[39m=\u001b[39m TensorDataset(test_features, test_targets)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/dataset.py:189\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr)\n",
    "train_targets = torch.Tensor(y_train_arr)\n",
    "val_features = torch.Tensor(X_val_arr)\n",
    "val_targets = torch.Tensor(y_val_arr)\n",
    "test_features = torch.Tensor(X_test_arr)\n",
    "test_targets = torch.Tensor(y_test_arr)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
