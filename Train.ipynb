{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Symbol                 Date       Open      Close       High  \\\n",
      "0          RPD  2018-01-02 00:00:00  18.660000  19.010000  19.090000   \n",
      "1          RPD  2018-01-03 00:00:00  19.040001  19.350000  19.650000   \n",
      "2          RPD  2018-01-04 00:00:00  19.389999  19.980000  20.000000   \n",
      "3          RPD  2018-01-05 00:00:00  20.000000  20.010000  20.100000   \n",
      "4          RPD  2018-01-08 00:00:00  20.020000  20.350000  20.500000   \n",
      "...        ...                  ...        ...        ...        ...   \n",
      "5177034    RNG  2023-03-14 00:00:00  32.180000  31.590000  32.680000   \n",
      "5177035    OGI  2023-03-15 00:00:00   0.650000   0.640000   0.651000   \n",
      "5177036    RNG  2023-03-15 00:00:00  31.320000  32.380001  32.549999   \n",
      "5177037    OGI  2023-03-16 00:00:00   0.640000   0.649000   0.667000   \n",
      "5177038    RNG  2023-03-16 00:00:00  32.419998  31.969999  32.580002   \n",
      "\n",
      "               Low     Volume   AdjClose  \n",
      "0        18.500000   124200.0  19.010000  \n",
      "1        19.040001   204100.0  19.350000  \n",
      "2        19.389999   177900.0  19.980000  \n",
      "3        19.719999   204400.0  20.010000  \n",
      "4        19.950001   237100.0  20.350000  \n",
      "...            ...        ...        ...  \n",
      "5177034  31.230000  2080300.0  31.590000  \n",
      "5177035   0.629000   822400.0   0.640000  \n",
      "5177036  30.990000  1694200.0  32.380001  \n",
      "5177037   0.629000  1386100.0   0.649000  \n",
      "5177038  31.180000  1495200.0  31.969999  \n",
      "\n",
      "[5177039 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('history.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Symbol       Date       Open      Close       High        Low  \\\n",
      "0          RPD 2018-01-02  18.660000  19.010000  19.090000  18.500000   \n",
      "1          RPD 2018-01-03  19.040001  19.350000  19.650000  19.040001   \n",
      "2          RPD 2018-01-04  19.389999  19.980000  20.000000  19.389999   \n",
      "3          RPD 2018-01-05  20.000000  20.010000  20.100000  19.719999   \n",
      "4          RPD 2018-01-08  20.020000  20.350000  20.500000  19.950001   \n",
      "...        ...        ...        ...        ...        ...        ...   \n",
      "5177034    RNG 2023-03-14  32.180000  31.590000  32.680000  31.230000   \n",
      "5177035    OGI 2023-03-15   0.650000   0.640000   0.651000   0.629000   \n",
      "5177036    RNG 2023-03-15  31.320000  32.380001  32.549999  30.990000   \n",
      "5177037    OGI 2023-03-16   0.640000   0.649000   0.667000   0.629000   \n",
      "5177038    RNG 2023-03-16  32.419998  31.969999  32.580002  31.180000   \n",
      "\n",
      "            Volume   AdjClose  Year  Month  DayOfWeek  WeekOfYear  \n",
      "0         124200.0  19.010000  2018      1          1           1  \n",
      "1         204100.0  19.350000  2018      1          2           1  \n",
      "2         177900.0  19.980000  2018      1          3           1  \n",
      "3         204400.0  20.010000  2018      1          4           1  \n",
      "4         237100.0  20.350000  2018      1          0           2  \n",
      "...            ...        ...   ...    ...        ...         ...  \n",
      "5177034  2080300.0  31.590000  2023      3          1          11  \n",
      "5177035   822400.0   0.640000  2023      3          2          11  \n",
      "5177036  1694200.0  32.380001  2023      3          2          11  \n",
      "5177037  1386100.0   0.649000  2023      3          3          11  \n",
      "5177038  1495200.0  31.969999  2023      3          3          11  \n",
      "\n",
      "[5177039 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Add in special date columns\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df.apply(lambda row: row['Date'].year, axis=1)\n",
    "df['Month'] = df.apply(lambda row: row['Date'].month, axis=1)\n",
    "df['DayOfWeek'] = df.apply(lambda row: row['Date'].weekday(), axis=1)\n",
    "df['WeekOfYear'] = df.apply(lambda row: row['Date'].isocalendar()[1], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Symbol', 'Date', 'Open', 'Close', 'High', 'Low', 'Volume', 'AdjClose',\n",
      "       'Year', 'Month', 'DayOfWeek', 'WeekOfYear', 'sin_DayOfWeek',\n",
      "       'cos_DayOfWeek', 'sin_Month', 'cos_Month', 'sin_WeekOfYear',\n",
      "       'cos_WeekOfYear'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_cyclical_features(df, col_name, period, start_num=0):\n",
    "    kwargs = {\n",
    "        f'sin_{col_name}' : lambda x: np.sin(2*np.pi*(df[col_name]-start_num)/period),\n",
    "        f'cos_{col_name}' : lambda x: np.cos(2*np.pi*(df[col_name]-start_num)/period)    \n",
    "             }\n",
    "    return df.assign(**kwargs)\n",
    "\n",
    "df = generate_cyclical_features(df, 'DayOfWeek', 7, 0)\n",
    "df = generate_cyclical_features(df, 'Month', 12, 1)\n",
    "df = generate_cyclical_features(df, 'WeekOfYear', 52, 0)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Symbol', 'Date', 'Open', 'Close', 'High', 'Low', 'Volume', 'AdjClose',\n",
      "       'Year', 'Month', 'DayOfWeek', 'WeekOfYear', 'sin_DayOfWeek',\n",
      "       'cos_DayOfWeek', 'sin_Month', 'cos_Month', 'sin_WeekOfYear',\n",
      "       'cos_WeekOfYear', 'is_holiday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import holidays\n",
    "us_holidays = holidays.US()\n",
    "\n",
    "def is_holiday(date):\n",
    "    date = date.replace(hour = 0)\n",
    "    return 1 if (date in us_holidays) else 0\n",
    "\n",
    "def add_holiday_col(df, holidays):\n",
    "    return df.assign(is_holiday = df['Date'].apply(is_holiday))\n",
    "\n",
    "df = add_holiday_col(df, us_holidays)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert symbol to number\n",
    "symbol_map = {}\n",
    "symbol_map_rev = {}\n",
    "\n",
    "symbols = df['Symbol'].unique()\n",
    "for i, symbol in enumerate(symbols):\n",
    "    symbol_map[symbol] = i\n",
    "    symbol_map_rev[i] = symbol\n",
    "\n",
    "df['Symbol_Num'] = df.apply(lambda row: symbol_map[row['Symbol']], axis=1)\n",
    "df = df.drop('Symbol', axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Split into Training and Test sets\n",
    "\n",
    "Do this by breaking data up into date ranges, as we are mirroing a live setup where we have the full \n",
    "past data, and need to predict the future. We are not trying to do things like, given random selctions\n",
    "of dates in the past predict a future date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750141 3332\n",
      "351138 3421\n",
      "372819 3523\n"
     ]
    }
   ],
   "source": [
    "# cols_to_drop = ['Month', 'Date', 'DayOfWeek', 'WeekOfYear']\n",
    "\n",
    "# train_x = df[(df['Year']==2018) & (df['Month'] < 12)]\n",
    "# i1 = train_x.set_index(['Symbol_Num']).index\n",
    "# train_x = train_x.drop(cols_to_drop, axis=1)\n",
    "# train_y = df[(df['Year']==2018) & (df['Month'] == 12)]\n",
    "# i2 = train_y.set_index(['Symbol_Num']).index\n",
    "# train_y = train_y[i2.isin(i1)]\n",
    "# train_y = train_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "# print(len(train_x), len(train_y))\n",
    "\n",
    "# val_x = df[(df['Year']==2019) & (df['Month'] < 6)]\n",
    "# i1 = val_x.set_index(['Symbol_Num']).index\n",
    "# val_x = val_x.drop(cols_to_drop, axis=1)\n",
    "# val_y = df[(df['Year']==2019) & (df['Month'] == 6)]\n",
    "# i2 = val_y.set_index(['Symbol_Num']).index\n",
    "# val_y = val_y[i2.isin(i1)]\n",
    "# val_y = val_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "# print(len(val_x), len(val_y))\n",
    "\n",
    "\n",
    "# test_x = df[(df['Year']==2019) & (df['Month'] < 12) & (df['Month'] > 6)]\n",
    "# i1 = test_x.set_index(['Symbol_Num']).index\n",
    "# test_x = test_x.drop(cols_to_drop, axis=1).groupby('Symbol_Num')\n",
    "# test_y = df[(df['Year']==2019) & (df['Month'] == 12)]\n",
    "# i2 = test_y.set_index(['Symbol_Num']).index\n",
    "# test_y = test_y[i2.isin(i1)]\n",
    "# test_y = test_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "# print(len(test_x), len(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Month', 'Date', 'DayOfWeek', 'WeekOfYear']\n",
    "\n",
    "train_x = df[(df['Year']==2018) & (df['Month'] < 12)].drop(cols_to_drop, axis=1)\n",
    "train_y = df[(df['Year']==2018) & (df['Month'] == 12)].drop(cols_to_drop, axis=1)\n",
    "\n",
    "val_x = df[(df['Year']==2019) & (df['Month'] < 6)].drop(cols_to_drop, axis=1)\n",
    "val_y = df[(df['Year']==2019) & (df['Month'] == 6)].drop(cols_to_drop, axis=1)\n",
    "\n",
    "test_x = df[(df['Year']==2019) & (df['Month'] < 12) & (df['Month'] > 6)].drop(cols_to_drop, axis=1)\n",
    "test_y = df[(df['Year']==2019) & (df['Month'] == 12)].drop(cols_to_drop, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# t = scaler.fit_transform(train_x)\n",
    "# print(t.shape)\n",
    "#X_train_arr = transfrom_group(train_x, scaler)\n",
    "# X_val_arr = transfrom_group(val_x, scaler)\n",
    "# X_test_arr = transfrom_group(test_x, scaler)\n",
    "\n",
    "\n",
    "# y_train_arr = scaler.fit_transform(train_y)\n",
    "# y_val_arr = scaler.transform(val_y)\n",
    "# y_test_arr = scaler.transform(test_y)\n",
    "\n",
    "# print(len(X_train_arr), len(y_train_arr))\n",
    "# print(len(X_val_arr), len(y_val_arr))\n",
    "# print(len(X_test_arr), len(y_test_arr))\n",
    "\n",
    "#TODO: This doesnt work right as valuations and test will not have same symbol num,\n",
    "#we probably need to figure out the right way to encode the symbol\n",
    "\n",
    "train_x_scaled = pd.DataFrame(scaler.fit_transform(train_x),columns = train_x.columns)\n",
    "train_y_scaled = pd.DataFrame(scaler.transform(train_y),columns = train_y.columns)\n",
    "\n",
    "val_x_scaled = pd.DataFrame(scaler.fit_transform(val_x),columns = val_x.columns)\n",
    "val_y_scaled = pd.DataFrame(scaler.transform(val_y),columns = val_y.columns)\n",
    "\n",
    "test_x_scaled = pd.DataFrame(scaler.fit_transform(test_x),columns = test_x.columns)\n",
    "test_y_scaled = pd.DataFrame(scaler.transform(test_y),columns = test_y.columns)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        y = y_data.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "        self.y_data = dict(zip(y.index, y[('Volume', 'mean')]))\n",
    "\n",
    "        symbols = x_data.groupby('Symbol_Num').groups.keys()\n",
    "        y_symbols = y_data.groupby('Symbol_Num').groups\n",
    "        self.symbol_nums = {}\n",
    "        count = 0\n",
    "        for key in symbols:\n",
    "            if key in y_symbols:\n",
    "                self.symbol_nums[count] = key\n",
    "                count += 1\n",
    "        # y = set(y_symbols.keys())\n",
    "        # x = set(self.symbol_nums.values())\n",
    "        # print(len(x.intersection(y)))\n",
    "        # print(-1.7371406599594579 in y)\n",
    "        self.len = count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        symbol = self.symbol_nums[idx]\n",
    "        y = self.y_data[symbol]\n",
    "        x_data = self.x_data[self.x_data[\"Symbol_Num\"]==symbol]\n",
    "        t = torch.tensor(x_data.values)\n",
    "        return t, y, len(x_data)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_x_scaled, train_y_scaled)\n",
    "val_dataset = TimeSeriesDataset(val_x_scaled, val_y_scaled)\n",
    "test_dataset = TimeSeriesDataset(test_x_scaled, test_y_scaled)\n",
    "\n",
    "def time_series_collate(batch):\n",
    "    # Pad sequences with zeros to make them the same length\n",
    "    return pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=time_series_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=time_series_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=time_series_collate)\n",
    "test_loader_one = DataLoader(test_dataset, batch_size=1, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0210, -0.0213, -0.0202,  ...,  1.6189, -0.0944, -1.7371],\n",
       "         [-0.0209, -0.0213, -0.0201,  ...,  1.6189, -0.0944, -1.7371],\n",
       "         [-0.0209, -0.0212, -0.0201,  ...,  1.6189, -0.0944, -1.7371],\n",
       "         ...,\n",
       "         [-0.0197, -0.0198, -0.0189,  ...,  1.4588, -0.0944, -1.7371],\n",
       "         [-0.0195, -0.0198, -0.0188,  ...,  1.4588, -0.0944, -1.7371],\n",
       "         [-0.0194, -0.0197, -0.0187,  ...,  1.4588, -0.0944, -1.7371]],\n",
       "        dtype=torch.float64),\n",
       " -0.1289282317971904,\n",
       " 232)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        #unpad \n",
    "        x = pack_padded_sequence(x, lengths)\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        #pad back to expected length\n",
    "        out, lengths = pad_packed_sequence(out)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train_step(self, x, y, lengths):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x, lengths)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch, lengths in train_loader:\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch, lengths)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val, lengths in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val, lengths)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test, lengths in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test, lengths)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(y_test.to(device).detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[251], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m     24\u001b[0m opt \u001b[39m=\u001b[39m Optimization(model\u001b[39m=\u001b[39mmodel, loss_fn\u001b[39m=\u001b[39mloss_fn, optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[0;32m---> 25\u001b[0m opt\u001b[39m.\u001b[39;49mtrain(train_loader, val_loader, batch_size\u001b[39m=\u001b[39;49mbatch_size, n_epochs\u001b[39m=\u001b[39;49mn_epochs, n_features\u001b[39m=\u001b[39;49minput_dim)\n\u001b[1;32m     26\u001b[0m opt\u001b[39m.\u001b[39mplot_losses()\n\u001b[1;32m     28\u001b[0m predictions, values \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39mevaluate(test_loader_one, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, n_features\u001b[39m=\u001b[39minput_dim)\n",
      "Cell \u001b[0;32mIn[250], line 39\u001b[0m, in \u001b[0;36mOptimization.train\u001b[0;34m(self, train_loader, val_loader, batch_size, n_epochs, n_features)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     38\u001b[0m     batch_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     40\u001b[0m         x_batch \u001b[39m=\u001b[39m x_batch\u001b[39m.\u001b[39mview([batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n_features])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m         y_batch \u001b[39m=\u001b[39m y_batch\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[245], line 42\u001b[0m, in \u001b[0;36mtime_series_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtime_series_collate\u001b[39m(batch):\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Pad sequences with zeros to make them the same length\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m pad_sequence(batch, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, padding_value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/utils/rnn.py:398\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    394\u001b[0m         sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39munbind(\u001b[39m0\u001b[39m)\n\u001b[1;32m    396\u001b[0m \u001b[39m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(train_x.columns)\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 2\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model_params = {'input_dim': input_dim,\n",
    "                'hidden_dim' : hidden_dim,\n",
    "                'layer_dim' : layer_dim,\n",
    "                'output_dim' : output_dim,\n",
    "                'dropout_prob' : dropout}\n",
    "\n",
    "model = get_model('gru', model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
