{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Symbol                 Date       Open      Close       High  \\\n",
      "0          RPD  2018-01-02 00:00:00  18.660000  19.010000  19.090000   \n",
      "1          RPD  2018-01-03 00:00:00  19.040001  19.350000  19.650000   \n",
      "2          RPD  2018-01-04 00:00:00  19.389999  19.980000  20.000000   \n",
      "3          RPD  2018-01-05 00:00:00  20.000000  20.010000  20.100000   \n",
      "4          RPD  2018-01-08 00:00:00  20.020000  20.350000  20.500000   \n",
      "...        ...                  ...        ...        ...        ...   \n",
      "5177034    RNG  2023-03-14 00:00:00  32.180000  31.590000  32.680000   \n",
      "5177035    OGI  2023-03-15 00:00:00   0.650000   0.640000   0.651000   \n",
      "5177036    RNG  2023-03-15 00:00:00  31.320000  32.380001  32.549999   \n",
      "5177037    OGI  2023-03-16 00:00:00   0.640000   0.649000   0.667000   \n",
      "5177038    RNG  2023-03-16 00:00:00  32.419998  31.969999  32.580002   \n",
      "\n",
      "               Low     Volume   AdjClose  \n",
      "0        18.500000   124200.0  19.010000  \n",
      "1        19.040001   204100.0  19.350000  \n",
      "2        19.389999   177900.0  19.980000  \n",
      "3        19.719999   204400.0  20.010000  \n",
      "4        19.950001   237100.0  20.350000  \n",
      "...            ...        ...        ...  \n",
      "5177034  31.230000  2080300.0  31.590000  \n",
      "5177035   0.629000   822400.0   0.640000  \n",
      "5177036  30.990000  1694200.0  32.380001  \n",
      "5177037   0.629000  1386100.0   0.649000  \n",
      "5177038  31.180000  1495200.0  31.969999  \n",
      "\n",
      "[5177039 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#TODO: add in extra datasets \n",
    "df = pd.read_csv('history.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Symbol       Date       Open      Close       High        Low  \\\n",
      "0          RPD 2018-01-02  18.660000  19.010000  19.090000  18.500000   \n",
      "1          RPD 2018-01-03  19.040001  19.350000  19.650000  19.040001   \n",
      "2          RPD 2018-01-04  19.389999  19.980000  20.000000  19.389999   \n",
      "3          RPD 2018-01-05  20.000000  20.010000  20.100000  19.719999   \n",
      "4          RPD 2018-01-08  20.020000  20.350000  20.500000  19.950001   \n",
      "...        ...        ...        ...        ...        ...        ...   \n",
      "5177034    RNG 2023-03-14  32.180000  31.590000  32.680000  31.230000   \n",
      "5177035    OGI 2023-03-15   0.650000   0.640000   0.651000   0.629000   \n",
      "5177036    RNG 2023-03-15  31.320000  32.380001  32.549999  30.990000   \n",
      "5177037    OGI 2023-03-16   0.640000   0.649000   0.667000   0.629000   \n",
      "5177038    RNG 2023-03-16  32.419998  31.969999  32.580002  31.180000   \n",
      "\n",
      "            Volume   AdjClose  Year  Month  DayOfWeek  WeekOfYear  \n",
      "0         124200.0  19.010000  2018      1          1           1  \n",
      "1         204100.0  19.350000  2018      1          2           1  \n",
      "2         177900.0  19.980000  2018      1          3           1  \n",
      "3         204400.0  20.010000  2018      1          4           1  \n",
      "4         237100.0  20.350000  2018      1          0           2  \n",
      "...            ...        ...   ...    ...        ...         ...  \n",
      "5177034  2080300.0  31.590000  2023      3          1          11  \n",
      "5177035   822400.0   0.640000  2023      3          2          11  \n",
      "5177036  1694200.0  32.380001  2023      3          2          11  \n",
      "5177037  1386100.0   0.649000  2023      3          3          11  \n",
      "5177038  1495200.0  31.969999  2023      3          3          11  \n",
      "\n",
      "[5177039 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Add in special date columns\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df.apply(lambda row: row['Date'].year, axis=1)\n",
    "df['Month'] = df.apply(lambda row: row['Date'].month, axis=1)\n",
    "df['DayOfWeek'] = df.apply(lambda row: row['Date'].weekday(), axis=1)\n",
    "df['WeekOfYear'] = df.apply(lambda row: row['Date'].isocalendar()[1], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Symbol', 'Date', 'Open', 'Close', 'High', 'Low', 'Volume', 'AdjClose',\n",
      "       'Year', 'Month', 'DayOfWeek', 'WeekOfYear', 'sin_DayOfWeek',\n",
      "       'cos_DayOfWeek', 'sin_Month', 'cos_Month', 'sin_WeekOfYear',\n",
      "       'cos_WeekOfYear'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_cyclical_features(df, col_name, period, start_num=0):\n",
    "    kwargs = {\n",
    "        f'sin_{col_name}' : lambda x: np.sin(2*np.pi*(df[col_name]-start_num)/period),\n",
    "        f'cos_{col_name}' : lambda x: np.cos(2*np.pi*(df[col_name]-start_num)/period)    \n",
    "             }\n",
    "    return df.assign(**kwargs)\n",
    "\n",
    "df = generate_cyclical_features(df, 'DayOfWeek', 7, 0)\n",
    "df = generate_cyclical_features(df, 'Month', 12, 1)\n",
    "df = generate_cyclical_features(df, 'WeekOfYear', 52, 0)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Symbol', 'Date', 'Open', 'Close', 'High', 'Low', 'Volume', 'AdjClose',\n",
      "       'Year', 'Month', 'DayOfWeek', 'WeekOfYear', 'sin_DayOfWeek',\n",
      "       'cos_DayOfWeek', 'sin_Month', 'cos_Month', 'sin_WeekOfYear',\n",
      "       'cos_WeekOfYear', 'is_holiday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import holidays\n",
    "us_holidays = holidays.US()\n",
    "\n",
    "def is_holiday(date):\n",
    "    date = date.replace(hour = 0)\n",
    "    return 1 if (date in us_holidays) else 0\n",
    "\n",
    "def add_holiday_col(df, holidays):\n",
    "    return df.assign(is_holiday = df['Date'].apply(is_holiday))\n",
    "\n",
    "df = add_holiday_col(df, us_holidays)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert symbol to number\n",
    "#TODO: This probably is not the best way to convert symbol to a numeric feature\n",
    "symbol_map = {}\n",
    "symbol_map_rev = {}\n",
    "\n",
    "symbols = df['Symbol'].unique()\n",
    "for i, symbol in enumerate(symbols):\n",
    "    symbol_map[symbol] = i\n",
    "    symbol_map_rev[i] = symbol\n",
    "\n",
    "df['Symbol_Num'] = df.apply(lambda row: symbol_map[row['Symbol']], axis=1)\n",
    "df = df.drop('Symbol', axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Split into Training and Test sets\n",
    "\n",
    "Do this by breaking data up into date ranges, as we are mirroing a live setup where we have the full \n",
    "past data, and need to predict the future. We are not trying to do things like, given random selctions\n",
    "of dates in the past predict a future date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750141 3332\n",
      "351138 3421\n",
      "372819 3523\n"
     ]
    }
   ],
   "source": [
    "# cols_to_drop = ['Month', 'Date', 'DayOfWeek', 'WeekOfYear']\n",
    "\n",
    "# train_x = df[(df['Year']==2018) & (df['Month'] < 12)]\n",
    "# i1 = train_x.set_index(['Symbol_Num']).index\n",
    "# train_x = train_x.drop(cols_to_drop, axis=1)\n",
    "# train_y = df[(df['Year']==2018) & (df['Month'] == 12)]\n",
    "# i2 = train_y.set_index(['Symbol_Num']).index\n",
    "# train_y = train_y[i2.isin(i1)]\n",
    "# train_y = train_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "# print(len(train_x), len(train_y))\n",
    "\n",
    "# val_x = df[(df['Year']==2019) & (df['Month'] < 6)]\n",
    "# i1 = val_x.set_index(['Symbol_Num']).index\n",
    "# val_x = val_x.drop(cols_to_drop, axis=1)\n",
    "# val_y = df[(df['Year']==2019) & (df['Month'] == 6)]\n",
    "# i2 = val_y.set_index(['Symbol_Num']).index\n",
    "# val_y = val_y[i2.isin(i1)]\n",
    "# val_y = val_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "# print(len(val_x), len(val_y))\n",
    "\n",
    "\n",
    "# test_x = df[(df['Year']==2019) & (df['Month'] < 12) & (df['Month'] > 6)]\n",
    "# i1 = test_x.set_index(['Symbol_Num']).index\n",
    "# test_x = test_x.drop(cols_to_drop, axis=1).groupby('Symbol_Num')\n",
    "# test_y = df[(df['Year']==2019) & (df['Month'] == 12)]\n",
    "# i2 = test_y.set_index(['Symbol_Num']).index\n",
    "# test_y = test_y[i2.isin(i1)]\n",
    "# test_y = test_y.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "\n",
    "# print(len(test_x), len(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Month', 'Date', 'DayOfWeek', 'WeekOfYear']\n",
    "\n",
    "train_x = df[(df['Year']==2018) & (df['Month'] < 12)].drop(cols_to_drop, axis=1)\n",
    "train_y = df[(df['Year']==2018) & (df['Month'] == 12)].drop(cols_to_drop, axis=1)\n",
    "\n",
    "val_x = df[(df['Year']==2019) & (df['Month'] < 6)].drop(cols_to_drop, axis=1)\n",
    "val_y = df[(df['Year']==2019) & (df['Month'] == 6)].drop(cols_to_drop, axis=1)\n",
    "\n",
    "test_x = df[(df['Year']==2019) & (df['Month'] < 12) & (df['Month'] > 6)].drop(cols_to_drop, axis=1)\n",
    "test_y = df[(df['Year']==2019) & (df['Month'] == 12)].drop(cols_to_drop, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#TODO: Would other types of scalers make a differnce\n",
    "scaler = StandardScaler()\n",
    "# t = scaler.fit_transform(train_x)\n",
    "# print(t.shape)\n",
    "#X_train_arr = transfrom_group(train_x, scaler)\n",
    "# X_val_arr = transfrom_group(val_x, scaler)\n",
    "# X_test_arr = transfrom_group(test_x, scaler)\n",
    "\n",
    "\n",
    "# y_train_arr = scaler.fit_transform(train_y)\n",
    "# y_val_arr = scaler.transform(val_y)\n",
    "# y_test_arr = scaler.transform(test_y)\n",
    "\n",
    "# print(len(X_train_arr), len(y_train_arr))\n",
    "# print(len(X_val_arr), len(y_val_arr))\n",
    "# print(len(X_test_arr), len(y_test_arr))\n",
    "\n",
    "#TODO: This doesnt work right as valuations and test will not have same symbol num,\n",
    "#we probably need to figure out the right way to encode the symbol\n",
    "\n",
    "train_x_scaled = pd.DataFrame(scaler.fit_transform(train_x),columns = train_x.columns)\n",
    "train_y_scaled = pd.DataFrame(scaler.transform(train_y),columns = train_y.columns)\n",
    "\n",
    "val_x_scaled = pd.DataFrame(scaler.fit_transform(val_x),columns = val_x.columns)\n",
    "val_y_scaled = pd.DataFrame(scaler.transform(val_y),columns = val_y.columns)\n",
    "\n",
    "test_x_scaled = pd.DataFrame(scaler.fit_transform(test_x),columns = test_x.columns)\n",
    "test_y_scaled = pd.DataFrame(scaler.transform(test_y),columns = test_y.columns)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from numpy import argsort\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        y = y_data.groupby('Symbol_Num').agg({'Volume':[\"mean\"]})\n",
    "        self.y_data = dict(zip(y.index, y[('Volume', 'mean')]))\n",
    "\n",
    "        symbols = x_data.groupby('Symbol_Num').groups.keys()\n",
    "        y_symbols = y_data.groupby('Symbol_Num').groups\n",
    "        self.symbol_nums = {}\n",
    "        count = 0\n",
    "        for key in symbols:\n",
    "            if key in y_symbols:\n",
    "                self.symbol_nums[count] = key\n",
    "                count += 1\n",
    "        # y = set(y_symbols.keys())\n",
    "        # x = set(self.symbol_nums.values())\n",
    "        # print(len(x.intersection(y)))\n",
    "        # print(-1.7371406599594579 in y)\n",
    "        self.len = count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        symbol = self.symbol_nums[idx]\n",
    "        y = self.y_data[symbol]\n",
    "        x_data = self.x_data[self.x_data[\"Symbol_Num\"]==symbol]\n",
    "        t = torch.tensor(x_data.values, dtype=torch.float32) #cast to float32 for faster computation\n",
    "        return t, y, len(x_data)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_x_scaled, train_y_scaled)\n",
    "val_dataset = TimeSeriesDataset(val_x_scaled, val_y_scaled)\n",
    "test_dataset = TimeSeriesDataset(test_x_scaled, test_y_scaled)\n",
    "\n",
    "def time_series_collate(batch):\n",
    "    #print(len(batch[0]))\n",
    "    sequence = []\n",
    "    y_vals = []\n",
    "    lengths = []\n",
    "    #pack padded zeros requires inputs to be sorted by length\n",
    "    sorted_indexes = list(reversed(argsort(list(map(lambda x: x[2], batch)))))\n",
    "    for i in sorted_indexes:\n",
    "        sequence.append(batch[i][0])\n",
    "        y_vals.append(batch[i][1])\n",
    "        lengths.append(batch[i][2])\n",
    "    # Pad sequences with zeros to make them the same length\n",
    "    return pad_sequence(sequence, batch_first=True, padding_value=0), torch.Tensor(y_vals), torch.Tensor(lengths)\n",
    "\n",
    "#TODO: Is there a better approach here than just dropping the last batch\n",
    "#TODO: Weights and Balances - optimize batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=time_series_collate, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=time_series_collate, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=time_series_collate, drop_last=True)\n",
    "test_loader_one = DataLoader(test_dataset, batch_size=1, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.0216, -0.0220, -0.0208,  ...,  1.6189, -0.0944, -1.7038],\n",
      "         [-0.0216, -0.0220, -0.0208,  ...,  1.6189, -0.0944, -1.7038],\n",
      "         [-0.0216, -0.0220, -0.0208,  ...,  1.6189, -0.0944, -1.7038],\n",
      "         ...,\n",
      "         [-0.0216, -0.0220, -0.0208,  ...,  1.4588, -0.0944, -1.7038],\n",
      "         [-0.0216, -0.0220, -0.0208,  ...,  1.4588, -0.0944, -1.7038],\n",
      "         [-0.0216, -0.0219, -0.0207,  ...,  1.4588, -0.0944, -1.7038]],\n",
      "\n",
      "        [[-0.0152, -0.0152, -0.0148,  ...,  1.6189, -0.0944, -1.7218],\n",
      "         [-0.0151, -0.0154, -0.0148,  ...,  1.6189, -0.0944, -1.7218],\n",
      "         [-0.0151, -0.0153, -0.0149,  ...,  1.6189, -0.0944, -1.7218],\n",
      "         ...,\n",
      "         [-0.0166, -0.0168, -0.0162,  ...,  1.4588, -0.0944, -1.7218],\n",
      "         [-0.0166, -0.0167, -0.0160,  ...,  1.4588, -0.0944, -1.7218],\n",
      "         [-0.0165, -0.0167, -0.0161,  ...,  1.4588, -0.0944, -1.7218]],\n",
      "\n",
      "        [[-0.0184, -0.0186, -0.0176,  ...,  1.6189, -0.0944, -1.7086],\n",
      "         [-0.0183, -0.0187, -0.0178,  ...,  1.6189, -0.0944, -1.7086],\n",
      "         [-0.0184, -0.0188, -0.0178,  ...,  1.6189, -0.0944, -1.7086],\n",
      "         ...,\n",
      "         [-0.0181, -0.0183, -0.0174,  ...,  1.4588, -0.0944, -1.7086],\n",
      "         [-0.0181, -0.0184, -0.0174,  ...,  1.4588, -0.0944, -1.7086],\n",
      "         [-0.0182, -0.0183, -0.0176,  ...,  1.4588, -0.0944, -1.7086]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0215, -0.0219, -0.0206,  ..., -0.3926, -0.0944, -1.6968],\n",
      "         [-0.0215, -0.0217, -0.0206,  ..., -0.3926, -0.0944, -1.6968],\n",
      "         [-0.0214, -0.0217, -0.0205,  ..., -0.5570, -0.0944, -1.6968],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0208, -0.0215, -0.0199,  ..., -1.3128, -0.0944, -1.6961],\n",
      "         [-0.0211, -0.0214, -0.0202,  ..., -1.3453, -0.0944, -1.6961],\n",
      "         [-0.0210, -0.0212, -0.0200,  ..., -1.3453, -0.0944, -1.6961],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0212, -0.0216, -0.0204,  ..., -1.3453, -0.0944, -1.6746],\n",
      "         [-0.0212, -0.0216, -0.0204,  ..., -1.3453, -0.0944, -1.6746],\n",
      "         [-0.0212, -0.0216, -0.0204,  ..., -1.3453, -0.0944, -1.6746],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), tensor([-0.1081, -0.1700, -0.2144, -0.2137,  4.8449, -0.0146, -0.1707, -0.2120,\n",
      "        -0.1705, -0.2074, -0.1300, -0.1628,  0.1617, -0.2147, -0.1699, -0.2138,\n",
      "        -0.0445, -0.1395, -0.2145, -0.0283, -0.0836, -0.2122, -0.2101, -0.1087,\n",
      "        -0.2138, -0.2152, -0.2118, -0.0223, -0.2153, -0.1496, -0.1979, -0.1289,\n",
      "        -0.2116,  0.0541,  0.0197, -0.2086,  1.1946,  0.3944, -0.2153, -0.1782,\n",
      "        -0.2135,  0.0070,  1.4180, -0.1978, -0.1899,  0.1733,  1.3467, -0.1073,\n",
      "        -0.0949, -0.0494,  0.0810, -0.1638, -0.0998, -0.2060, -0.2132, -0.2147,\n",
      "         0.0694, -0.1321, -0.2138, -0.1958, -0.2108,  0.0692, -0.2073, -0.2131]), tensor([232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232.,\n",
      "        232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232.,\n",
      "        232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232.,\n",
      "        232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232.,\n",
      "        232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 232., 216.,\n",
      "        211., 158., 118., 117.]))\n"
     ]
    }
   ],
   "source": [
    "#print(len(train_dataset))\n",
    "#train_dataset[0]\n",
    "\n",
    "for x in train_loader:\n",
    "    print(x)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        #unpad \n",
    "        x = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        \n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        #pad back to expected length\n",
    "        out, lengths = pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train_step(self, x, y, lengths):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        #print(x.shape)\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x, lengths)\n",
    "\n",
    "        #print(y.shape, yhat.shape)\n",
    "        #print(y)\n",
    "        #print(y, yhat)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            count = 0\n",
    "            for x_batch, y_batch, lengths in train_loader:\n",
    "                #TODO: What does this view do here?\n",
    "                #print(\"DIMS: \", x_batch.shape)\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.view([batch_size,-1]).to(device)\n",
    "                loss = self.train_step(x_batch, y_batch, lengths)\n",
    "                batch_losses.append(loss)\n",
    "                count += 1\n",
    "                if not count % 15:\n",
    "                    print(\"Epoch {}, Batch Num: {}, Loss: {}\".format(epoch, count, loss))\n",
    "\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val, lengths in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.view([batch_size,-1]).view([batch_size,-1]).to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val, lengths)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            print(\n",
    "                f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test, lengths in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.view([batch_size,-1]).to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test, lengths)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(y_test.to(device).detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "#TODO: get this working on GPU\n",
    "# Set the device\n",
    "#device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch Num: 15, Loss: 0.04971149191260338\n",
      "Epoch 1, Batch Num: 30, Loss: 0.07218178361654282\n",
      "Epoch 1, Batch Num: 45, Loss: 0.06321752071380615\n",
      "[1/50] Training loss: 0.8063\t Validation loss: 0.4071\n",
      "Epoch 2, Batch Num: 15, Loss: 0.16891087591648102\n",
      "Epoch 2, Batch Num: 30, Loss: 0.05495019257068634\n",
      "Epoch 2, Batch Num: 45, Loss: 0.04941292107105255\n",
      "[2/50] Training loss: 0.4649\t Validation loss: 1.1539\n",
      "Epoch 3, Batch Num: 15, Loss: 0.24924024939537048\n",
      "Epoch 3, Batch Num: 30, Loss: 0.07842231541872025\n",
      "Epoch 3, Batch Num: 45, Loss: 0.09238702803850174\n",
      "[3/50] Training loss: 0.4643\t Validation loss: 19.1185\n",
      "Epoch 4, Batch Num: 15, Loss: 0.1737200915813446\n",
      "Epoch 4, Batch Num: 30, Loss: 0.05090923607349396\n",
      "Epoch 4, Batch Num: 45, Loss: 0.03391371667385101\n",
      "[4/50] Training loss: 0.3361\t Validation loss: 3.6489\n",
      "Epoch 5, Batch Num: 15, Loss: 0.13246743381023407\n",
      "Epoch 5, Batch Num: 30, Loss: 0.12506148219108582\n",
      "Epoch 5, Batch Num: 45, Loss: 0.05945517122745514\n",
      "[5/50] Training loss: 0.3286\t Validation loss: 11.9822\n",
      "Epoch 6, Batch Num: 15, Loss: 0.049673885107040405\n",
      "Epoch 6, Batch Num: 30, Loss: 0.104975126683712\n",
      "Epoch 6, Batch Num: 45, Loss: 0.015965096652507782\n",
      "[6/50] Training loss: 0.2319\t Validation loss: 4.1605\n",
      "Epoch 7, Batch Num: 15, Loss: 0.0828426256775856\n",
      "Epoch 7, Batch Num: 30, Loss: 0.038099437952041626\n",
      "Epoch 7, Batch Num: 45, Loss: 0.03207357972860336\n",
      "[7/50] Training loss: 0.1326\t Validation loss: 2.4231\n",
      "Epoch 8, Batch Num: 15, Loss: 0.09421824663877487\n",
      "Epoch 8, Batch Num: 30, Loss: 0.033882446587085724\n",
      "Epoch 8, Batch Num: 45, Loss: 0.012098433449864388\n",
      "[8/50] Training loss: 0.1517\t Validation loss: 4.8650\n",
      "Epoch 9, Batch Num: 15, Loss: 0.016995465382933617\n",
      "Epoch 9, Batch Num: 30, Loss: 0.012637821026146412\n",
      "Epoch 9, Batch Num: 45, Loss: 0.054757941514253616\n",
      "[9/50] Training loss: 0.2459\t Validation loss: 0.8838\n",
      "Epoch 10, Batch Num: 15, Loss: 0.05096547305583954\n",
      "Epoch 10, Batch Num: 30, Loss: 0.10570549219846725\n",
      "Epoch 10, Batch Num: 45, Loss: 0.02591511979699135\n",
      "[10/50] Training loss: 0.2522\t Validation loss: 3.2469\n",
      "Epoch 11, Batch Num: 15, Loss: 0.044831711798906326\n",
      "Epoch 11, Batch Num: 30, Loss: 0.31832247972488403\n",
      "Epoch 11, Batch Num: 45, Loss: 0.08665919303894043\n",
      "Epoch 12, Batch Num: 15, Loss: 0.24045230448246002\n",
      "Epoch 12, Batch Num: 30, Loss: 0.07973991334438324\n",
      "Epoch 12, Batch Num: 45, Loss: 0.025458822026848793\n",
      "Epoch 13, Batch Num: 15, Loss: 0.2766876220703125\n",
      "Epoch 13, Batch Num: 30, Loss: 0.039795614778995514\n",
      "Epoch 13, Batch Num: 45, Loss: 0.02774365060031414\n",
      "Epoch 14, Batch Num: 15, Loss: 0.1389881670475006\n",
      "Epoch 14, Batch Num: 30, Loss: 0.05764438956975937\n",
      "Epoch 14, Batch Num: 45, Loss: 0.052403151988983154\n",
      "Epoch 15, Batch Num: 15, Loss: 0.060931652784347534\n",
      "Epoch 15, Batch Num: 30, Loss: 0.044933538883924484\n",
      "Epoch 15, Batch Num: 45, Loss: 0.028338246047496796\n",
      "Epoch 16, Batch Num: 15, Loss: 0.08551185578107834\n",
      "Epoch 16, Batch Num: 30, Loss: 0.06379299610853195\n",
      "Epoch 16, Batch Num: 45, Loss: 0.025728903710842133\n",
      "Epoch 17, Batch Num: 15, Loss: 0.10963697731494904\n",
      "Epoch 17, Batch Num: 30, Loss: 0.02853514812886715\n",
      "Epoch 17, Batch Num: 45, Loss: 0.021039443090558052\n",
      "Epoch 18, Batch Num: 15, Loss: 0.15578293800354004\n",
      "Epoch 18, Batch Num: 30, Loss: 0.020093204453587532\n",
      "Epoch 18, Batch Num: 45, Loss: 0.014446486718952656\n",
      "Epoch 19, Batch Num: 15, Loss: 0.034519877284765244\n",
      "Epoch 19, Batch Num: 30, Loss: 0.032684069126844406\n",
      "Epoch 19, Batch Num: 45, Loss: 0.02831973135471344\n",
      "Epoch 20, Batch Num: 15, Loss: 0.10483040660619736\n",
      "Epoch 20, Batch Num: 30, Loss: 0.027803948149085045\n",
      "Epoch 20, Batch Num: 45, Loss: 0.03245195373892784\n",
      "Epoch 21, Batch Num: 15, Loss: 0.046639807522296906\n",
      "Epoch 21, Batch Num: 30, Loss: 0.0239863358438015\n",
      "Epoch 21, Batch Num: 45, Loss: 0.04050091654062271\n",
      "Epoch 22, Batch Num: 15, Loss: 0.06861687451601028\n",
      "Epoch 22, Batch Num: 30, Loss: 0.02635248936712742\n",
      "Epoch 22, Batch Num: 45, Loss: 0.0295869167894125\n",
      "Epoch 23, Batch Num: 15, Loss: 0.07099216431379318\n",
      "Epoch 23, Batch Num: 30, Loss: 0.036483023315668106\n",
      "Epoch 23, Batch Num: 45, Loss: 0.034521348774433136\n",
      "Epoch 24, Batch Num: 15, Loss: 0.07970449328422546\n",
      "Epoch 24, Batch Num: 30, Loss: 0.015582074411213398\n",
      "Epoch 24, Batch Num: 45, Loss: 0.04473923519253731\n",
      "Epoch 25, Batch Num: 15, Loss: 0.06684119999408722\n",
      "Epoch 25, Batch Num: 30, Loss: 0.031541988253593445\n",
      "Epoch 25, Batch Num: 45, Loss: 0.027453618124127388\n",
      "Epoch 26, Batch Num: 15, Loss: 0.059716202318668365\n",
      "Epoch 26, Batch Num: 30, Loss: 0.028551967814564705\n",
      "Epoch 26, Batch Num: 45, Loss: 0.0329272598028183\n",
      "Epoch 27, Batch Num: 15, Loss: 0.12315920740365982\n",
      "Epoch 27, Batch Num: 30, Loss: 0.043317701667547226\n",
      "Epoch 27, Batch Num: 45, Loss: 0.030655521899461746\n",
      "Epoch 28, Batch Num: 15, Loss: 0.05581449717283249\n",
      "Epoch 28, Batch Num: 30, Loss: 0.01761019416153431\n",
      "Epoch 28, Batch Num: 45, Loss: 0.015928076580166817\n",
      "Epoch 29, Batch Num: 15, Loss: 0.01880188100039959\n",
      "Epoch 29, Batch Num: 30, Loss: 0.023038437590003014\n",
      "Epoch 29, Batch Num: 45, Loss: 0.01739881932735443\n",
      "Epoch 30, Batch Num: 15, Loss: 0.023811517283320427\n",
      "Epoch 30, Batch Num: 30, Loss: 0.034352704882621765\n",
      "Epoch 30, Batch Num: 45, Loss: 0.03282671794295311\n",
      "Epoch 31, Batch Num: 15, Loss: 0.09365615248680115\n",
      "Epoch 31, Batch Num: 30, Loss: 0.030264176428318024\n",
      "Epoch 31, Batch Num: 45, Loss: 0.020335327833890915\n",
      "Epoch 32, Batch Num: 15, Loss: 0.016458237543702126\n",
      "Epoch 32, Batch Num: 30, Loss: 0.016528621315956116\n",
      "Epoch 32, Batch Num: 45, Loss: 0.019794821739196777\n",
      "Epoch 33, Batch Num: 15, Loss: 0.033273544162511826\n",
      "Epoch 33, Batch Num: 30, Loss: 0.033501025289297104\n",
      "Epoch 33, Batch Num: 45, Loss: 0.02572883665561676\n",
      "Epoch 34, Batch Num: 15, Loss: 0.02202569507062435\n",
      "Epoch 34, Batch Num: 30, Loss: 0.009653731249272823\n",
      "Epoch 34, Batch Num: 45, Loss: 0.029782840982079506\n",
      "Epoch 35, Batch Num: 15, Loss: 0.021235091611742973\n",
      "Epoch 35, Batch Num: 30, Loss: 0.029674293473362923\n",
      "Epoch 35, Batch Num: 45, Loss: 0.018662508577108383\n",
      "Epoch 36, Batch Num: 15, Loss: 0.031961310654878616\n",
      "Epoch 36, Batch Num: 30, Loss: 0.012132485397160053\n",
      "Epoch 36, Batch Num: 45, Loss: 0.029848657548427582\n",
      "Epoch 37, Batch Num: 15, Loss: 0.06212933734059334\n",
      "Epoch 37, Batch Num: 30, Loss: 0.023262806236743927\n",
      "Epoch 37, Batch Num: 45, Loss: 0.020853951573371887\n",
      "Epoch 38, Batch Num: 15, Loss: 0.05188863351941109\n",
      "Epoch 38, Batch Num: 30, Loss: 0.023174429312348366\n",
      "Epoch 38, Batch Num: 45, Loss: 0.02406974881887436\n",
      "Epoch 39, Batch Num: 15, Loss: 0.04005589708685875\n",
      "Epoch 39, Batch Num: 30, Loss: 0.009839896112680435\n",
      "Epoch 39, Batch Num: 45, Loss: 0.01654002070426941\n",
      "Epoch 40, Batch Num: 15, Loss: 0.023117024451494217\n",
      "Epoch 40, Batch Num: 30, Loss: 0.01605418138206005\n",
      "Epoch 40, Batch Num: 45, Loss: 0.02207222953438759\n",
      "Epoch 41, Batch Num: 15, Loss: 0.03444232791662216\n",
      "Epoch 41, Batch Num: 30, Loss: 0.012907114811241627\n",
      "Epoch 41, Batch Num: 45, Loss: 0.017988480627536774\n",
      "Epoch 42, Batch Num: 15, Loss: 0.022055858746170998\n",
      "Epoch 42, Batch Num: 30, Loss: 0.037184447050094604\n",
      "Epoch 42, Batch Num: 45, Loss: 0.015928808599710464\n",
      "Epoch 43, Batch Num: 15, Loss: 0.026327650994062424\n",
      "Epoch 43, Batch Num: 30, Loss: 0.03218298405408859\n",
      "Epoch 43, Batch Num: 45, Loss: 0.0325932614505291\n",
      "Epoch 44, Batch Num: 15, Loss: 0.041581761091947556\n",
      "Epoch 44, Batch Num: 30, Loss: 0.027226705104112625\n",
      "Epoch 44, Batch Num: 45, Loss: 0.005106098018586636\n",
      "Epoch 45, Batch Num: 15, Loss: 0.035206139087677\n",
      "Epoch 45, Batch Num: 30, Loss: 0.00774756632745266\n",
      "Epoch 45, Batch Num: 45, Loss: 0.025722118094563484\n",
      "Epoch 46, Batch Num: 15, Loss: 0.02475913241505623\n",
      "Epoch 46, Batch Num: 30, Loss: 0.008637410588562489\n",
      "Epoch 46, Batch Num: 45, Loss: 0.02031189203262329\n",
      "Epoch 47, Batch Num: 15, Loss: 0.015266270376741886\n",
      "Epoch 47, Batch Num: 30, Loss: 0.03937351703643799\n",
      "Epoch 47, Batch Num: 45, Loss: 0.028687985613942146\n",
      "Epoch 48, Batch Num: 15, Loss: 0.08445702493190765\n",
      "Epoch 48, Batch Num: 30, Loss: 0.03676862269639969\n",
      "Epoch 48, Batch Num: 45, Loss: 0.022525154054164886\n",
      "Epoch 49, Batch Num: 15, Loss: 0.026685714721679688\n",
      "Epoch 49, Batch Num: 30, Loss: 0.011106998659670353\n",
      "Epoch 49, Batch Num: 45, Loss: 0.013576764613389969\n",
      "Epoch 50, Batch Num: 15, Loss: 0.027311261743307114\n",
      "Epoch 50, Batch Num: 30, Loss: 0.008840776979923248\n",
      "Epoch 50, Batch Num: 45, Loss: 0.0180478785187006\n",
      "[50/50] Training loss: 0.0960\t Validation loss: 0.1378\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#TODO: Optimize these paramaters - Weights and Balances\n",
    "input_dim = len(train_x.columns)\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 50\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-6\n",
    "\n",
    "\n",
    "model_params = {'input_dim': input_dim,\n",
    "                'hidden_dim' : hidden_dim,\n",
    "                'layer_dim' : layer_dim,\n",
    "                'output_dim' : output_dim,\n",
    "                'dropout_prob' : dropout}\n",
    "\n",
    "#TODO: Try other Models  - Weights and Balances\n",
    "model = get_model('gru', model_params).to(device)\n",
    "\n",
    "#TODO: Try other loss functions and optimizers - Weights and Balances\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#TODO: Optimize training performance\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n",
    "\n",
    "#TODO: Plot actual predictions vs averages to see how we did\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
